{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124cc0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import unidecode\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a891376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pasta base (diretório onde está este script)\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "FILE_PATH_DATASET = os.path.join(BASE_DIR, 'datasets', 'RECLAMEAQUI_HAPVIDA.csv')\n",
    "#FILE_PATH_SHP = os.path.join(BASE_DIR, 'datasets', 'BR_Municipios_2024.shp')\n",
    "\n",
    "# O arquivo final será o mesmo tratado\n",
    "FILE_PATH_OUTPUT = os.path.join(BASE_DIR, 'datasets', 'RECLAMEAQUI_HAPVIDA_treat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    #Carrega o dataset original.\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep=',', encoding='utf-8')\n",
    "    return df\n",
    "\n",
    "def split_city_state(df, column='LOCAL'):\n",
    "    # Divide a coluna \"LOCAL\" no formato 'Cidade - UF' em duas colunas separadas: 'Cidade' e 'UF'.\n",
    "    \n",
    "    if column in df.columns:\n",
    "        split_cols = df[column].str.split(' - ', expand=True)\n",
    "        df['Cidade'] = split_cols[0]\n",
    "        df['UF'] = split_cols[1]\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "def padronizar_nome(texto):\n",
    "    #Padroniza nomes removendo acentos e deixando em maiúsculo.\n",
    "    \n",
    "    return unidecode.unidecode(str(texto)).strip().upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aeb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Leitura do CSV original\n",
    "print(f\"Lendo arquivo: {FILE_PATH_DATASET}\")\n",
    "df = load_data(FILE_PATH_DATASET)\n",
    "print(f\"Dataset carregado com {df.shape[0]} linhas e {df.shape[1]} colunas.\")\n",
    "dff = df.copy()\n",
    "\n",
    "# Dividindo coluna LOCAL por CIDADE e ESTADO.\n",
    "dff[['CIDADE', 'ESTADO']] = dff['LOCAL'].str.split(' - ', expand=True)\n",
    "\n",
    "# Gerando tamanho do texto\n",
    "dff['TAMANHO_TEXTO'] = dff['DESCRICAO'].str.len()\n",
    "\n",
    "# removendo duas linas que não tem estado\n",
    "dff = dff[~(dff['ESTADO'] == '--')]\n",
    "\n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac39fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b632c1",
   "metadata": {},
   "source": [
    "# Figure Serie Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import processamento\n",
    "\n",
    "\n",
    "fig_serie_temporal = px.line(\n",
    "        #dff.groupby('MES').size().reset_index(name='CONTAGEM'), \n",
    "        processamento.DataProcessing(df=df).series_temporal(),\n",
    "        x='MES', \n",
    "        y='CONTAGEM',\n",
    "        title='Série Temporal de Reclamações', \n",
    "        labels={'MES': 'Mês', 'CONTAGEM': 'Nº Reclamações'}\n",
    "    ).update_layout(title_x=0.5, margin=dict(t=50, l=10, r=10, b=10))\n",
    "\n",
    "fig_serie_temporal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f934e2b",
   "metadata": {},
   "source": [
    "# Figure reclamações por estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8356fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_estado_data = px.bar(\n",
    "        #dff['ESTADO'].value_counts().reset_index(),\n",
    "        processamento.DataProcessing(df=df).data_estado(),\n",
    "        x='ESTADO', \n",
    "        y='count', \n",
    "        title='Reclamações por Estado', \n",
    "        labels={'ESTADO': 'Estado', 'count': 'Nº Reclamações'}, \n",
    "        text_auto=True\n",
    "    ).update_xaxes(categoryorder=\"total descending\").update_layout(title_x=0.5, margin=dict(t=50, l=10, r=10, b=10))\n",
    "\n",
    "fig_estado_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a554ab",
   "metadata": {},
   "source": [
    "# Figure distribuição por status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb816e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_status_data = px.pie(\n",
    "        #dff['STATUS'].value_counts().reset_index(),\n",
    "        processamento.DataProcessing(df=df).data_status(),\n",
    "        names='STATUS', \n",
    "        values='count', \n",
    "        title='Distribuição por Status', \n",
    "        hole=0.4\n",
    "    ).update_layout(title_x=0.5, margin=dict(t=50, l=10, r=10, b=10))\n",
    "\n",
    "fig_status_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938b131",
   "metadata": {},
   "source": [
    "# Figure distribuição tamanho do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig_dist_texto = px.histogram(    \n",
    "        #dff, \n",
    "        processamento.DataProcessing(df=df).data_texto(),\n",
    "        x='TAMANHO_TEXTO', \n",
    "        title='Distribuição do Tamanho do Texto',\n",
    "        labels={'TAMANHO_TEXTO': 'Tamanho do Texto', 'count': 'Frequência'}\n",
    "    ).update_layout(title_x=0.5, margin=dict(t=50, l=10, r=10, b=10))\n",
    "fig_dist_texto.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e7f77",
   "metadata": {},
   "source": [
    "# Figure mapa de reclamações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e256cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para completar os estados que não tem dados no dataframe\n",
    " \n",
    "dict_estados = {\n",
    "    'AC': 'Acre', \n",
    "    'AL': 'Alagoas', \n",
    "    'AP': 'Amapá', \n",
    "    'AM': 'Amazonas', \n",
    "    'BA': 'Bahia',\n",
    "    'CE': 'Ceará',\n",
    "    'DF': 'Distrito Federal', \n",
    "    'ES': 'Espírito Santo', \n",
    "    'GO': 'Goiás',\n",
    "    'MA': 'Maranhão', \n",
    "    'MT': 'Mato Grosso', \n",
    "    'MS': 'Mato Grosso do Sul', \n",
    "    'MG': 'Minas Gerais',\n",
    "    'PA': 'Pará', \n",
    "    'PB': 'Paraíba', \n",
    "    'PR': 'Paraná', \n",
    "    'PE': 'Pernambuco', \n",
    "    'PI': 'Piauí',\n",
    "    'RJ': 'Rio de Janeiro',\n",
    "    'RN': 'Rio Grande do Norte', \n",
    "    'RS': 'Rio Grande do Sul',\n",
    "    'RO': 'Rondônia', \n",
    "    'RR': 'Roraima', \n",
    "    'SC': 'Santa Catarina', \n",
    "    'SP': 'São Paulo',\n",
    "    'SE': 'Sergipe', \n",
    "    'TO': 'Tocantins'\n",
    "}\n",
    "\n",
    "ano_mapa = 2022\n",
    "# Salva um dataframe para tratamento dos dados\n",
    "\n",
    "df_contagem_estado = dff[dff['ANO'] == ano_mapa].groupby('ESTADO').size().reset_index(name='CONTAGEM')\n",
    "\n",
    "# Adiciona a silga e contagem em zero do estado que não tem no dataframe\n",
    "\n",
    "for sigla in dict_estados.keys():\n",
    "    if sigla not in set(df_contagem_estado['ESTADO'].unique()):\n",
    "        df_contagem_estado.loc[len(df_contagem_estado)] = {'ESTADO': sigla, 'CONTAGEM': 0}\n",
    "\n",
    "# Adiciona dos nomes dos estados.\n",
    "#         \n",
    "df_contagem_estado['NOME'] = df_contagem_estado['ESTADO'].map(dict_estados)\n",
    "df_contagem_estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import processamento\n",
    "\n",
    "ano_mapa = 2022\n",
    "\n",
    "#caminho_arquivo = './datasets/brasil_estados.json'\n",
    "\n",
    "#with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "#    dados = json.load(f)      \n",
    "    \n",
    "\n",
    "fig_map_data = px.choropleth_mapbox(\n",
    "    #dff[dff['ANO'] == ano_mapa].groupby('ESTADO').size().reset_index(name='CONTAGEM'),\n",
    "    #df_contagem_estado,\n",
    "    #geojson=dados,\n",
    "    processamento.DataProcessing(df=df).data_mapa(ano_mapa=ano_mapa),\n",
    "    geojson=processamento.DataProcessing.open_geo_json(),\n",
    "    locations='ESTADO',     \n",
    "    color='CONTAGEM', \n",
    "    color_continuous_scale=\"reds\",\n",
    "    mapbox_style=\"carto-positron\", \n",
    "    zoom=3.2, \n",
    "    center={\"lat\": -14.2350, \"lon\": -51.9253},\n",
    "    title=f'Reclamações em {ano_mapa}', \n",
    "    labels={'CONTAGEM': 'Nº Reclamações'}\n",
    ").update_layout(title_x=0.5, margin=dict(t=50, l=0, r=0, b=0))\n",
    "\n",
    "fig_map_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# Obter a lista de stopwords em português\n",
    "stopwords_portugues = stopwords.words('portuguese')\n",
    "\n",
    "stopwords_planos_saude = {\n",
    "    # Termos Gerais de Planos de Saúde\n",
    "    'plano', 'planos', 'saúde', 'convênio', 'convênios', 'seguro', 'seguros',\n",
    "    'operadora', 'operadoras', 'benefício', 'benefícios', 'serviço', 'serviços',\n",
    "    'contrato', 'contratos', 'apólice', 'apólices', 'cobertura', 'coberturas',\n",
    "    'rede', 'redes', 'credenciada', 'credenciadas', 'atendimento', 'atendimentos',\n",
    "    'assistência', 'assistências', 'médico', 'médica', 'médicos', 'médicas',\n",
    "    'hospital', 'hospitais', 'clínica', 'clínicas', 'exame', 'exames', 'consulta',\n",
    "    'consultas', 'procedimento', 'procedimentos', 'reembolso', 'reembolsos',\n",
    "    'carência', 'carências', 'mensalidade', 'mensalidades', 'custo', 'custos',\n",
    "    'preço', 'preços', 'valor', 'valores', 'pagamento', 'pagamentos', 'titular',\n",
    "    'dependentes', 'adesão', 'portabilidade', 'urgência', 'emergência', 'ans',\n",
    "    'agência', 'nacional',\n",
    "\n",
    "    # Termos Relacionados/Descritivos Comuns - podem não ser o foco\n",
    "    'cliente', 'clientes', 'usuário', 'usuários', 'paciente', 'pacientes',\n",
    "    'doutor', 'doutora', 'doutores', 'doutoras', 'equipe', 'equipes',\n",
    "    'empresa', 'empresas', 'filho', 'filha', 'vc',    \n",
    "    'bom', 'boa', 'bons', 'boas', 'ruim', 'ruins', 'ótimo', 'ótima',\n",
    "    'excelente', 'péssimo', 'péssima', 'muito', 'pouco', 'mais', 'menos',\n",
    "    'assim', 'porém', 'entanto', 'também', 'ainda', 'já', 'sempre', 'nunca',\n",
    "    'alguns', 'algumas', 'todo', 'toda', 'todos', 'todas', \"pois\", \"outro\", \n",
    "    \"outra\", \"dia\", \"dias\", \"entrega\", \"reclame\", \"aqui\", \"problema\",\n",
    "    'q', 'fiz', ',', 'Hapvida', \"não\", 'nao', \"pra\", \"tive\", \"minha\", 'contato',\n",
    "    'fazer', 'nada', 'ter', 'preciso', 'ano', 'onde', 'vai', 'após',\n",
    "    'hoje', 'SAC', 'vez', 'pode', 'lá', 'por que', 'porque','pq', 'faz', 'vou',\n",
    "    'quero', 'disse', 'data', 'então', 'ir', 'desde', 'sendo', 'agora',\n",
    "    'meses', 'entrei', 'consegui', 'passar', 'pessoa', 'entrar', 'semana',\n",
    "    'sobre', 'horário', 'hora', 'mês', 'min', 'ja', 'mesma', 'pago', 'passou',\n",
    "    'bem', 'atendente', 'bem', 'anos', 'liguei', 'mail', 'saber', 'ligar',\n",
    "    'quase', 'mim', 'apenas','muito', 'pouco', 'mais', 'menos', 'bastante', \n",
    "    'quase', 'tão', 'tão', 'demais', 'suficiente', 'suficientes', 'totalmente', \n",
    "    'parcialmente', 'completamente', 'absolutamente', 'relativamente', \n",
    "    'excessivamente', 'apenas', 'só','bom', 'boa', 'bons', 'boas', 'ruim', 'ruins', \n",
    "    'excelente', 'excelentes', 'péssimo', 'péssima', 'péssimos', 'péssimas',\n",
    "    'melhor', 'melhores', 'pior', 'piores', 'grande', 'grandes', 'pequeno', 'pequena',\n",
    "    'pequenos', 'pequenas', 'certo', 'certa', 'certos', 'certas', 'errado', 'errada',\n",
    "    'errados', 'erradas', 'positivo', 'positiva', 'positivos', 'positivas',\n",
    "    'negativo', 'negativa', 'negativos', 'negativas', 'legal', 'legais',\n",
    "    'incrível', 'incríveis', 'horrível', 'horríveis', 'razoável', 'razoáveis',\n",
    "    'bom', 'má', 'maus', 'más', 'grande', 'pequeno', 'novo', 'nova', 'novos', \n",
    "    'novas', 'velho', 'velha', 'velhos', 'velhas', 'ótimo', 'ótima', 'ótimos', 'ótimas',    \n",
    "    'sempre', 'nunca', 'jamais', 'às vezes', 'frequentemente', 'raramente', 'constantemente',\n",
    "    'diariamente', 'semanalmente', 'mensalmente', 'anualmente', 'hoje', 'ontem', 'amanhã',\n",
    "    'agora', 'depois', 'antes', 'cedo', 'tarde', 'logo', 'já', 'ainda', 'durante',    \n",
    "    'assim', 'bem', 'mal', 'melhor', 'pior', 'rapidamente', 'lentamente', 'facilmente',\n",
    "    'dificilmente', 'geralmente', 'especialmente', 'principalmente', 'claro', 'clara',\n",
    "    'claramente', 'verdadeiro', 'verdadeira', 'verdadeiros', 'verdadeiras', 'falso',\n",
    "    'falsa', 'falsos', 'falsas', 'possível', 'possíveis', 'impossível', 'impossíveis',    \n",
    "    'porém', 'contudo', 'entretanto', 'todavia', 'assim', 'portanto', 'logo', 'daí',\n",
    "    'além', 'ainda', 'inclusive', 'mesmo', 'embora', 'apesar', 'conforme', 'segundo',\n",
    "    'exceto', 'salvo', 'inclusive', 'além disso', 'em vez de', 'ao invés de', 'ou seja',    \n",
    "    'aqui', 'lá', 'ali', 'cá', 'perto', 'longe', 'dentro', 'fora', 'em cima', 'em baixo',\n",
    "    'atrás', 'frente', 'lado', 'ao lado', 'direita', 'esquerda', 'meio',    \n",
    "    'próprio', 'própria', 'próprios', 'próprias', 'outro', 'outra', 'outros', 'outras',\n",
    "    'mesmo', 'mesma', 'mesmos', 'mesmas', 'todo', 'toda', 'todos', 'todas',\n",
    "    'algum', 'alguma', 'alguns', 'algumas', 'nenhum', 'nenhuma', 'nenhuns', 'nenhumas',\n",
    "    'vários', 'várias', 'diversos', 'diversas', 'certo', 'certa', 'certos', 'certas',\n",
    "    'tal', 'tais', 'qualquer', 'quaisquer', 'cujo', 'cuja', 'cujos', 'cujas',\n",
    "    'primeiro', 'primeira', 'segundo', 'segunda', 'último', 'última',\n",
    "    'único', 'única', 'únicos', 'únicas', 'sistema', 'atende', 'momento', 'telefone',\n",
    "    'vezes'\n",
    "}\n",
    "\n",
    "for palavra in stopwords_planos_saude:\n",
    "    stopwords_portugues.append(palavra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5451b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \" \".join(dff['DESCRICAO'].astype(str).tolist())\n",
    "print(len(texto)) \n",
    "print(texto[:100])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white',\n",
    "    stopwords=stopwords_portugues,\n",
    "    colormap='viridis', \n",
    "    max_words=50\n",
    ").generate(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa001037",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "#plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.imshow(processamento.DataProcessing(df=df).data_wordcloud(), interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud - Termos mais frequentes nas reclamações')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dashboards-python-dash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
